import requests
import arxiv
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
import logging
import time
from tqdm import tqdm

from app.services.database import db
from app.models import Paper
from app.models.last_update import LastUpdate
from app.services.nlp_service import  summarize_abstract 


logger = logging.getLogger(__name__)

MAX_PAPER_COUNT = 100

BASE_URL = "http://export.arxiv.org/api/query"

ARXIV_CATEGORY_MAPPING = {
    "cs.AI": "Artificial Intelligence",
    "cs.LG": "Machine Learning",
    "cs.CV": "Computer Vision",
    "cs.CL": "Natural Language Processing",
    "cs.RO": "Robotics",
    "cs.NE": "Neural Networks",
    "cs.IR": "Information Retrieval",
    "cs.MA": "Multi-Agent Systems",
    "stat.ML": "Statistical Machine Learning",
}

def categorize_papers(arxiv_categories):
    """
    ë…¼ë¬¸ ë°ì´í„°ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    """
    subject_labels = set()

    for category in arxiv_categories.split():
        if category in ARXIV_CATEGORY_MAPPING:
            subject_labels.add(ARXIV_CATEGORY_MAPPING[category])

    return ", ".join(subject_labels) if subject_labels else "Other"

def fetch_and_save_papers():
    """
    Arxivì—ì„œ ë…¼ë¬¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ìž¥í•©ë‹ˆë‹¤.
    """
    categories = ARXIV_CATEGORY_MAPPING.keys()
    search_query = 'cat:' + ' OR cat:'.join(categories)
    one_week_ago = datetime.now(timezone.utc) - timedelta(days=31)  # UTC ê¸°ì¤€ ìµœê·¼ 7ì¼

    search = arxiv.Search(
        query=search_query,
        max_results=MAX_PAPER_COUNT,
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Descending
    )

    try:
        time.sleep(3)
        results = list(search.results())  # tqdmì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        print(f" ðŸ” {len(results)} papers found")
        if not results:  # âœ… API ì‘ë‹µì´ ë¹„ì–´ ìžˆëŠ” ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
            print("arXivì—ì„œ ê°€ì ¸ì˜¨ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. (ë¹ˆ ê²°ê³¼)")
            return

    except arxiv.UnexpectedEmptyPageError:
        print("âŒ arXiv API ì˜¤ë¥˜: ë¹ˆ íŽ˜ì´ì§€ê°€ ë°˜í™˜ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
        return
    
    added_count = 0
    skipped_count = 0

    print(f"ðŸ” {len(results)} papers found")

    for result in tqdm(results, desc="Adding papers", unit="paper"):
        time.sleep(3)  # âœ… ìš”ì²­ ì†ë„ ì œí•œ ì¶”ê°€

        # ë‚ ì§œë¥¼ í•„í„°ë§í•˜ì—¬ 1ì£¼ì¼ ì´ë‚´ ë°ì´í„°ë§Œ ì²˜ë¦¬
        if result.published >= one_week_ago:  # aware datetime ë¹„êµ
            
            existing_paper = Paper.query.filter_by(url=result.entry_id).first()

            if existing_paper:
                print(f"Already existed paper : {result.title} (URL: {result.entry_id})")
                skipped_count += 1
                continue

            domain_task = categorize_papers(result.primary_category)
            summary = summarize_abstract(result.summary)

            paper = Paper(
                title=result.title,
                abstract=result.summary,
                authors=', '.join([author.name for author in result.authors]),
                published_date=result.published,
                source='arXiv',
                url=result.entry_id.strip(),
                domain_task=domain_task,
                summary = summary,
            )
            try:
                db.session.add(paper)
                db.session.commit()
                added_count += 1
                print(f"âœ… paper added: {result.title}")
                print(f"summary: {summary}")
    
            except Exception as e:
                print(f"âŒ paper add failed: {result.title} (error: {str(e)})")
                db.session.rollback()
    
    # latest update time update
    try:
        print("ðŸ•’ Updating last_update timestamp")
        db.session.query(LastUpdate).delete()
        db.session.add(LastUpdate(updated_at=datetime.now()))
        print(f"âœ… Last update timestamp updated: {datetime.now()}")
    except Exception as e:
        print(f"âŒ Last update timestamp update failed: {str(e)}")
        db.session.rollback()

    db.session.commit()
    print(f"âœ… {added_count} papers are added")
    print(f"âŒ {skipped_count} papers are skipped")

    # delete old papers
    clean_old_papers()

def parse_arxiv_response(xml_data):
    """
    arXiv API ì‘ë‹µ XML ë°ì´í„°ë¥¼ íŒŒì‹±í•˜ì—¬ ë…¼ë¬¸ ì œëª©ê³¼ ì´ˆë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    root = ET.fromstring(xml_data)
    papers = []

    for paper in papers:
        logger.info(f"ë¶ˆëŸ¬ì˜¨ ë…¼ë¬¸ ì œëª©: {paper['title']}")

    for entry in root.findall("{http://www.w3.org/2005/Atom}entry"):
        title = entry.find("{http://www.w3.org/2005/Atom}title").text
        summary = entry.find("{http://www.w3.org/2005/Atom}summary").text
        papers.append({"title": title.strip(), "abstract": summary.strip()})

    return papers

def clean_old_papers():
    """
    Delete old papers from the database
    """
    total_papers = Paper.query.count()

    if total_papers > MAX_PAPER_COUNT:
        num_to_delete = total_papers - MAX_PAPER_COUNT
        old_papers = Paper.query.order_by(Paper.published_date.asc()).limit(num_to_delete).all()

        for paper in old_papers:
            db.session.delete(paper)
        db.session.commit()

        print(f"âœ… {num_to_delete} papers are deleted. {MAX_PAPER_COUNT} papers are remained.")
