import requests
import arxiv
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta, timezone
import logging
import time
from tqdm import tqdm

from app.services.database import db
from app.models import Paper
from app.services.nlp_service import classify_domain_task_with_model
from app.models.last_update import LastUpdate
from app.services.nlp_service import extract_keywords, summarize_long_text  # í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°


logger = logging.getLogger(__name__)

MAX_PAPER_COUNT = 500

BASE_URL = "http://export.arxiv.org/api/query"

ARXIV_CATEGORY_MAPPING = {
    "cs.AI": "Artificial Intelligence",
    "cs.LG": "Machine Learning",
    "cs.CV": "Computer Vision",
    "cs.CL": "Natural Language Processing",
    "cs.RO": "Robotics",
    "cs.NE": "Neural Networks",
    "cs.IR": "Information Retrieval",
    "cs.MA": "Multi-Agent Systems",
    "stat.ML": "Statistical Machine Learning",
}

def categorize_papers(arxiv_categories):
    """
    ë…¼ë¬¸ ë°ì´í„°ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    """
    subject_labels = set()

    for category in arxiv_categories.split():
        if category in ARXIV_CATEGORY_MAPPING:
            subject_labels.add(ARXIV_CATEGORY_MAPPING[category])

    return ", ".join(subject_labels) if subject_labels else "Other"

def fetch_and_save_papers():
    """
    Arxivì—ì„œ ë…¼ë¬¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    categories = ARXIV_CATEGORY_MAPPING.keys()
    search_query = 'cat:' + ' OR cat:'.join(categories)
    one_week_ago = datetime.now(timezone.utc) - timedelta(days=7)  # UTC ê¸°ì¤€ ìµœê·¼ 7ì¼

    search = arxiv.Search(
        query=search_query,
        max_results=MAX_PAPER_COUNT,
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Descending
    )

    try:
        time.sleep(3)
        results = list(search.results())  # tqdmì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        print(f" ğŸ” {len(results)} papers found")
        if not results:  # âœ… API ì‘ë‹µì´ ë¹„ì–´ ìˆëŠ” ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬
            print("arXivì—ì„œ ê°€ì ¸ì˜¨ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. (ë¹ˆ ê²°ê³¼)")
            return

    except arxiv.arxiv.UnexpectedEmptyPageError:
        print("âŒ arXiv API ì˜¤ë¥˜: ë¹ˆ í˜ì´ì§€ê°€ ë°˜í™˜ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
        return
    
    added_count = 0
    skipped_count = 0

    print(f"ğŸ” {len(results)} papers found")

    for result in tqdm(results, desc="Adding papers", unit="paper"):
        time.sleep(3)  # âœ… ìš”ì²­ ì†ë„ ì œí•œ ì¶”ê°€

        # ë‚ ì§œë¥¼ í•„í„°ë§í•˜ì—¬ 1ì£¼ì¼ ì´ë‚´ ë°ì´í„°ë§Œ ì²˜ë¦¬
        if result.published >= one_week_ago:  # aware datetime ë¹„êµ
            
            existing_paper = Paper.query.filter_by(url=result.entry_id).first()

            if existing_paper:
                print(f"Already existed paper : {result.title} (URL: {result.entry_id})")
                skipped_count += 1
                continue

            domain_task = categorize_papers(result.primary_category)
            keywords = extract_keywords(result.summary)
            summary = summarize_long_text(result.summary)

            paper = Paper(
                title=result.title,
                abstract=result.summary,
                summary = summary, 
                authors=', '.join([author.name for author in result.authors]),
                published_date=result.published,
                source='arXiv',
                url=result.entry_id.strip(),
                domain_task=domain_task,
                keywords=", ".join(keywords)
            )
            try:
                db.session.add(paper)
                db.session.commit()
                added_count += 1
                print(f"âœ… paper added: {result.title}")
    
            except Exception as e:
                print(f"âŒ paper add failed: {result.title} (error: {str(e)})")
                db.session.rollback()
    
    # latest update time update
    try:
        print("ğŸ•’ Updating last_update timestamp")
        db.session.query(LastUpdate).delete()
        db.session.add(LastUpdate(updated_at=datetime.now()))
        print(f"âœ… Last update timestamp updated: {datetime.now()}")
    except Exception as e:
        print(f"âŒ Last update timestamp update failed: {str(e)}")
        db.session.rollback()

    db.session.commit()
    print(f"âœ… {added_count} papers are added")
    print(f"âŒ {skipped_count} papers are skipped")

    # delete old papers
    clean_old_papers()

def update_domain_tasks_with_model():
    """
    ê¸°ì¡´ ë°ì´í„° ì¤‘ domain_taskê°€ ë¹„ì–´ìˆëŠ” ë…¼ë¬¸ì„ ì°¾ì•„ ë¶„ë¥˜ í›„ ì—…ë°ì´íŠ¸
    """
    papers = Paper.query.filter((Paper.domain_task == None) | (Paper.domain_task == "")).all()

    for paper in tqdm(papers, desc="Updating missing domain tasks"):
        paper.domain_task = classify_domain_task_with_model(paper.title, paper.abstract)
    
    db.session.commit()
    print(f"âœ… Updated {len(papers)} papers with missing domain_task.")

def parse_arxiv_response(xml_data):
    """
    arXiv API ì‘ë‹µ XML ë°ì´í„°ë¥¼ íŒŒì‹±í•˜ì—¬ ë…¼ë¬¸ ì œëª©ê³¼ ì´ˆë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    root = ET.fromstring(xml_data)
    papers = []

    for paper in papers:
        logger.info(f"ë¶ˆëŸ¬ì˜¨ ë…¼ë¬¸ ì œëª©: {paper['title']}")

    for entry in root.findall("{http://www.w3.org/2005/Atom}entry"):
        title = entry.find("{http://www.w3.org/2005/Atom}title").text
        summary = entry.find("{http://www.w3.org/2005/Atom}summary").text
        papers.append({"title": title.strip(), "abstract": summary.strip()})

    return papers

def clean_old_papers():
    """
    Delete old papers from the database
    """
    total_papers = Paper.query.count()

    if total_papers > MAX_PAPER_COUNT:
        num_to_delete = total_papers - MAX_PAPER_COUNT
        old_papers = Paper.query.order_by(Paper.published_date.asc()).limit(num_to_delete).all()

        for paper in old_papers:
            db.session.delete(paper)
        db.session.commit()

        print(f"âœ… {num_to_delete} papers are deleted. {MAX_PAPER_COUNT} papers are remained.")

def update_missing_paper_data():
    papers_missing_summary = Paper.query.filter((Paper.summary == None) | (Paper.summary == "")).all()
    papers_missing_keywords = Paper.query.filter((Paper.keywords == None) | (Paper.keywords == "")).all()
    papers_missing_labels = Paper.query.filter((Paper.subject_label == None) | (Paper.subject_label == "")).all()

    print(f"ğŸ“Œ Missing data: {len(papers_missing_summary)} summaries, {len(papers_missing_keywords)} keywords, {len(papers_missing_labels)} labels")

    for paper in tqdm(papers_missing_summary, desc="Updating missing summaries"):
        paper.summary = summarize_long_text(paper.abstract)
        
    for paper in tqdm(papers_missing_keywords, desc="Updating missing keywords"):
        paper.keywords = ", ".join(extract_keywords(paper.abstract))

    for paper in tqdm(papers_missing_labels, desc="Updating missing subject labels"):
        paper.subject_label = classify_domain_task_with_model(paper.title, paper.abstract)

    db.session.commit()
    print(f"âœ… Missing data updates completed.")

'''def fetch_latest_papers(query="artificial intelligence", max_results=50, last_days=None):
    """
    arXivì—ì„œ ìµœì‹  ë…¼ë¬¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. 
    ë‚ ì§œ í•„í„°ë§ ê¸°ëŠ¥ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.

    Args:
        query (str): ê²€ìƒ‰í•  ì¿¼ë¦¬.
        max_results (int): ìµœëŒ€ ë…¼ë¬¸ ê°œìˆ˜.
        last_days (int, optional): ìµœê·¼ ëª‡ ì¼ ë™ì•ˆì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ì§€.

    Returns:
        list: ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸.
    """
    params = {
        "search_query": f"all:{query}",
        "start": 0,
        "max_results": max_results,
        "sortBy": "submittedDate",
        "sortOrder": "descending",
    }

    # ë‚ ì§œ í•„í„°ë§
    if last_days:
        # í˜„ì¬ UTC ì‹œê°„ì—ì„œ last_daysë§Œí¼ ì´ì „ì˜ ë‚ ì§œ ê³„ì‚°
        start_date = (datetime.now(timezone.utc) - timedelta(days=last_days)).strftime('%Y%m%d%H%M%S')
        # submittedDate í•„ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë‚ ì§œ ë²”ìœ„ ì§€ì •
        params["search_query"] += f" AND submittedDate:[{start_date} TO *]"

    response = requests.get(BASE_URL, params=params)
    if response.status_code == 200:
        return parse_arxiv_response(response.text)
    else:
        return []'''